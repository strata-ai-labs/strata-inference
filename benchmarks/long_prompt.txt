The history of artificial intelligence began in antiquity, with myths, stories and rumors of artificial beings endowed with intelligence or consciousness by master craftsmen. The seeds of modern AI were planted by philosophers who attempted to describe the process of human thinking as the mechanical manipulation of symbols. This work culminated in the invention of the programmable digital computer in the 1940s, a machine based on the abstract essence of mathematical reasoning. This device and the ideas behind it inspired a handful of scientists to begin seriously discussing the possibility of building an electronic brain. The field of AI research was founded at a workshop held on the campus of Dartmouth College, USA during the summer of 1956. Those who attended would become the leaders of AI research for decades. Many of them predicted that a machine as intelligent as a human being would exist in no more than a generation, and they were given millions of dollars to make this vision come true. Eventually, it became obvious that commercial developers and researchers had grossly underestimated the difficulty of the project. In 1973, in response to the criticism from James Lighthill and ongoing pressure from congress, the U.S. and British governments stopped funding undirected research into artificial intelligence, and the difficult years that followed would later be known as an AI winter. Seven years later, a visionary initiative by the Japanese government inspired governments and industry to provide AI with billions of dollars, but by the late 1980s the investors became disillusioned by the absence of the needed computer power and withdrew funding again. Investment and interest in AI boomed in the first decades of the 21st century when machine learning was successfully applied to many problems in academia and industry due to new methods, the application of powerful computer hardware, and the collection of immense data sets. The field of deep learning expanded rapidly after 2012, leading to a revolution in natural language processing, computer vision, and many other domains. By 2025, generative artificial intelligence models such as large language models had achieved remarkable feats of reasoning and creativity, sparking intense debate about the nature of intelligence itself and the future of human civilization in a world increasingly shaped by thinking machines. Researchers around the globe continued to push boundaries, developing systems capable of scientific discovery, mathematical proof, code generation, and creative writing that rivaled or exceeded human performance in specific domains. Governments raced to establish regulatory frameworks while companies invested billions in scaling these systems further, driven by competitive pressure and the promise of transformative economic gains. The questions raised by these advances extended far beyond technology into philosophy, ethics, law, and social policy, challenging fundamental assumptions about what it means to be human in an age of artificial minds. Meanwhile, the quest for artificial general intelligence, a system with human-level cognitive abilities across all domains, remained an open and fiercely debated challenge, with estimates for its arrival ranging from years to decades to never. The practical applications of AI had already transformed daily life in countless ways: autonomous vehicles navigated city streets, AI assistants managed schedules and answered questions, recommendation algorithms shaped what people read and watched, and diagnostic systems helped doctors identify diseases with unprecedented accuracy. In scientific research, AI systems analyzed vast datasets from particle physics experiments, genomic sequencing projects, climate simulations, and astronomical surveys, accelerating the pace of discovery beyond what any human team could achieve alone. The economic implications were staggering, with some analysts predicting that AI would add trillions of dollars to global GDP while simultaneously displacing millions of workers from traditional employment. Educational institutions scrambled to redesign curricula to prepare students for a world where many cognitive tasks would be performed by machines, while philosophers and ethicists grappled with questions about consciousness, moral status, and the rights of artificial beings. The military applications of AI raised particularly urgent concerns, as autonomous weapons systems, cyber warfare tools, and intelligence analysis platforms became increasingly sophisticated and widely deployed. International efforts to establish norms and treaties governing the use of AI in warfare proceeded slowly, hampered by geopolitical rivalries and the dual-use nature of most AI technologies. In the arts, AI-generated music, visual art, literature, and film challenged traditional notions of creativity and authorship, leading to heated debates about intellectual property, attribution, and the value of human artistic expression in an era of machine-generated content. The intersection of AI with biotechnology opened new frontiers in drug discovery, personalized medicine, and genetic engineering, raising both hopes for medical breakthroughs and fears about biosecurity and the potential for misuse.